{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U segmentation-models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras==2.2.4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dropout, Flatten\nfrom tensorflow.keras.layers import Convolution2D,Activation,GlobalAveragePooling2D,MaxPooling2D,Flatten,Dense,Dropout,Input,Reshape,Lambda\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport os\nimport pandas as pd\nimport random\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\nfrom keras.models import Model\nimport math\nfrom keras.callbacks import TensorBoard\nimport tensorflow.keras.layers as L\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = [16, 8]\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport segmentation_models as sm\nfrom segmentation_models import PSPNet,Unet\nfrom segmentation_models.metrics import IOUScore, Precision,Recall\nfrom segmentation_models.losses import JaccardLoss,CategoricalFocalLoss,DiceLoss,CategoricalCELoss\nfrom tensorflow.keras.optimizers import RMSprop\n\nprint('Using Segmentation version:', sm.__version__)\nprint('Using Tensorflow version:', tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm.framework() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm.set_framework('tf.keras')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm.framework() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our setup, we:\n- created a data/ folder\n- created CDS folder which contains all folders\n- created 4 folders train_frames train_masks val_frames val_masks\n- created train and val subfolders inside 4 folders\n- put the pictures in data/CDS/train_frames/train/\n- put the label pictures in data/train_masks/train/\nIn summary, this is our directory structure:\n```\n    data/\n        CDS/\n            label_colors.txt\n            train_frames/\n                train/\n                    frame1.png # .png format\n            train_masks/\n                train/\n                    frame1_L.png # _L.png format\n            val_frames/\n                val/\n                    frame2.png # .png format\n            val_masks/\n                val/\n                    frame2_L.png # _L.png format\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_code(l):\n    '''Function to parse lines in a text file, returns separated elements (label codes and names in this case)\n    '''\n    if len(l.strip().split(\"\\t\")) == 2:\n        a, b = l.strip().split(\"\\t\")\n        return tuple(int(i) for i in a.split(' ')), b\n    else:\n        a, b, c = l.strip().split(\"\\t\")\n        return tuple(int(i) for i in a.split(' ')), c\n    \ndef rgb_to_onehot(rgb_image, colormap):\n    '''Function to one hot encode RGB mask labels\n        Inputs: \n            rgb_image - image matrix (eg. 144 x 144 x 3 dimension numpy ndarray)\n            colormap - dictionary of color to label id\n        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n    '''\n    num_classes = len(colormap)\n    shape = rgb_image.shape[:2]+(num_classes,)\n    encoded_image = np.zeros( shape, dtype=np.int32 )\n    for i, cls in enumerate(colormap):\n        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n    #print(encoded_image.shape)\n    return encoded_image\n\ndef onehot_to_rgb(onehot, colormap):\n    '''Function to decode encoded mask labels\n        Inputs: \n            onehot - one hot encoded image matrix (height x width x num_classes)\n            colormap - dictionary of color to label id\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros( onehot.shape[:2]+(3,) )\n    for k in colormap.keys():\n        output[single_layer==k] = colormap[k]\n    return np.uint8(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SIZE = (384,384)\ndef TrainAugmentGenerator(seed = 909, batch_size = 16):\n    '''Train Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    train_image_generator = train_frames_datagen.flow_from_directory(\n    DATA_PATH + 'train_frames/',\n    batch_size = batch_size, seed = seed,target_size=IMAGE_SIZE,shuffle=True,interpolation='nearest')\n\n    train_mask_generator = train_masks_datagen.flow_from_directory(\n    DATA_PATH + 'train_masks/',\n    batch_size = batch_size, seed = seed,target_size=IMAGE_SIZE,shuffle=True,interpolation='nearest')\n\n    while True:\n        X1i = train_image_generator.next()\n        X2i = train_mask_generator.next()\n        \n        # Add new for TPU\n#         for j in range(X1i[0].shape[0]):\n#             X1i[0][j,:,:,:] = tf.cast(X1i[0][j,:,:,:], tf.float32)/255.0\n            #print(X1i[0][j,:,:,:])\n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        #print(mask_encoded.shape)\n        yield X1i[0], np.asarray(mask_encoded)\n\ndef ValAugmentGenerator(seed = 909, batch_size = 16):\n    '''Validation Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    val_image_generator = val_frames_datagen.flow_from_directory(\n    DATA_PATH + 'val_frames/',\n    batch_size = batch_size, seed = seed,target_size=IMAGE_SIZE,interpolation='nearest',shuffle=False)\n\n\n    val_mask_generator = val_masks_datagen.flow_from_directory(\n    DATA_PATH + 'val_masks/',\n    batch_size = batch_size, seed = seed,target_size=IMAGE_SIZE,interpolation='nearest',shuffle=False)\n\n\n    while True:\n        X1i = val_image_generator.next()\n        X2i = val_mask_generator.next()\n        \n        # Add new for TPU\n#         for j in range(X1i[0].shape[0]):\n#             X1i[0][j,:,:,:] = tf.cast(X1i[0][j,:,:,:], tf.float32)/255.0\n            #print(X1i[0][j,:,:,:])\n        #print(X1i[0])\n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        \n        yield X1i[0], np.asarray(mask_encoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defind variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"img_dir = '../input/goodgame-data/GGDataSet/'\n\nDATA_PATH = '../input/goodgame-data/GGDataSet/'\n\n# Seed defined for aligning images and their masks\ntrain_numbers = len(os.listdir(img_dir + 'train_frames/train/'))\n\nval_numbers = len(os.listdir(img_dir + 'val_frames/val/'))\n\nprint('Number of train datasets: ',train_numbers)\n\nprint('Number of val datasets: ', val_numbers)\n\nBATCH_SIZE = 64\n\nEPOCHS = 10\n\nlabel_codes, label_names = zip(*[parse_code(l) for l in open(img_dir+\"label_colors.txt\")])\nlabel_codes, label_names = list(label_codes), list(label_names)\n\nprint(label_codes, label_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create useful label and code conversion dictionaries\nThese will be used for:\nOne hot encoding the mask labels for model training\nDecoding the predicted labels for interpretation and visualization\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"code2id = {v:k for k,v in enumerate(label_codes)}\nid2code = {k:v for k,v in enumerate(label_codes)}\n\nname2id = {v:k for k,v in enumerate(label_names)}\nid2name = {k:v for k,v in enumerate(label_names)}\n\nprint(id2code,id2name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Aug","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_gen_args = dict(rotation_range=0.2,\n                      width_shift_range=0.05,\n                      height_shift_range=0.05,\n                      shear_range=0.05,\n                      zoom_range=0.05,\n                      vertical_flip=True, #can than voi bien bao\n                      horizontal_flip=True,\n                      fill_mode='nearest',rescale=1./255) # If we use TPU we will use other method to resacle\n\nmask_gen_args = dict(rotation_range=0.2,\n                      width_shift_range=0.05,\n                      height_shift_range=0.05,\n                      shear_range=0.05,\n                      zoom_range=0.05,\n                      vertical_flip=True, #can than voi bien bao\n                      horizontal_flip=True,\n                      fill_mode='nearest')\n\ndata_gen_args_val = dict(rescale=1./255) # If we use TPU we will use other method to resacle\nmask_gen_args_val = dict()\n\ntrain_frames_datagen = ImageDataGenerator(**data_gen_args)\ntrain_masks_datagen = ImageDataGenerator(**mask_gen_args)\nval_frames_datagen = ImageDataGenerator(**data_gen_args_val)\nval_masks_datagen = ImageDataGenerator(**mask_gen_args_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate Weights","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Method 1\nWhen resize to (144,144) or (384,384) we need to calculate weights satify to image size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport os\nimport numba\n\n@numba.njit(fastmath=True)\ndef matrix(total_road,total_line,total_background,total_sign,image):\n    for j in range(0,image.shape[0]):\n        for k in range(0,image.shape[1]):\n            b,g,r = image[j][k]\n            if (r,g,b) == (128,0,0):\n                total_road += 1\n            if (r,g,b) == (192,128,128):\n                total_line += 1\n            if (r,g,b) == (0,0,0):\n                total_background +=1\n            if (r,g,b) == (102,102,102):\n                total_sign += 1\n    return total_road,total_line,total_background,total_sign\n\ndef calc_weights(masks_folder):\n    \"\"\"Calculate class weights according to classes distribution in a dataset\"\"\"\n    images_list = os.listdir(masks_folder)\n    total_road = 0\n    total_line = 0\n    total_background = 0\n    total_sign = 0\n    for i in range(len(images_list)):\n        print(\"Processing ---------\",round(i/len(images_list)*100,2),'%\\t',i,\"/\",len(images_list))\n        image = cv2.imread(masks_folder + '/' + images_list[i])\n        # Resize image to (144,144) or (384,384)\n        image = cv2.resize(image,IMAGE_SIZE,interpolation=cv2.INTER_NEAREST) # Noi suy \n        total_road,total_line,total_background,total_sign = matrix(total_road,total_line,total_background,total_sign,image)\n        #b,g,r = image[373][145]\n        #print(b,g,r)\n\n    total = len(images_list) * image.shape[0] * image.shape[1]\n    print(\"------------------------------------\")\n    print(\"Weight of Road: \", total_road / total)\n    print(\"Weight of Line: \", total_line / total)\n    print(\"Weight of BG: \", total_background / total)\n    print(\"Weight of Sign: \", total_sign / total)\n\nIMAGE_SIZE = (144,144)\nimg_dir = '../input/goodgame-data/GGDataSet/train_masks/train/'\n\ncalc_weights(img_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Method 2","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import cv2\nimport os\nimport numba\n\n@numba.njit(fastmath=True)\ndef matrix(total_road,total_line,total_background,total_sign,image):\n    for j in range(0,480):\n        for k in range(0,640):\n            b,g,r = image[j][k]\n            if (r,g,b) == (128,0,0):\n                total_road += 1\n            if (r,g,b) == (192,128,128):\n                total_line += 1\n            if (r,g,b) == (0,0,0):\n                total_background +=1\n            if (r,g,b) == (102,102,102):\n                total_sign += 1\n    return total_road,total_line,total_background,total_sign\n\n@numba.njit(fastmath=True)\ndef cal_contains_object(r_c,g_c,b_c,image):\n    for j in range(0,480):\n        for k in range(0,640):\n            b,g,r = image[j][k]\n            if (r,g,b) == (r_c,g_c,b_c):\n                return 1\n    return 0\ndef calc_weights(masks_folder):\n    \"\"\"Calculate class weights according to classes distribution in a dataset\"\"\"\n    global road_global\n    global line_global\n    global bg_global\n    global sign_global    \n    \n    images_list = os.listdir(masks_folder)\n    total_road = 0\n    total_line = 0\n    total_background = 0\n    total_sign = 0\n    for i in range(len(images_list)):\n        print(\"Processing ---------\",round(i/len(images_list)*100,2),'%\\t',i,\"/\",len(images_list))\n        image = cv2.imread(masks_folder + '/' + images_list[i])\n        total_road,total_line,total_background,total_sign = matrix(total_road,total_line,total_background,total_sign,image)\n        #b,g,r = image[373][145]\n        #print(b,g,r)\n        road_global += cal_contains_object(128,0,0,image)\n        line_global += cal_contains_object(192,128,128,image)\n        bg_global += cal_contains_object(0,0,0,image)\n        sign_global += cal_contains_object(102,102,102,image)\n        \n    print(\"Total of road images: \", road_global)\n    print(\"Total of line images: \", line_global)\n    print(\"Total of background images: \", bg_global)\n    print(\"Total of sign images: \", sign_global)\n\n    road_global = road_global * 640 * 480\n    line_global = line_global * 640 * 480\n    bg_global = bg_global * 640 * 480\n    sign_global = sign_global * 640 * 480\n\n    print(\"------------------------------------\")\n    print(\"Weight of Road: \", total_road / road_global)\n    print(\"Weight of Line: \", total_line / line_global)\n    print(\"Weight of BG: \", total_background / bg_global)\n    print(\"Weight of Sign: \", total_sign / sign_global)\n\nroad_global = 0\nline_global = 0\nbg_global = 0\nsign_global = 0\n\nimg_dir = '../input/goodgame-data/GGDataSet/train_frames/train'\n\ncalc_weights(img_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cusom loss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras.backend as K\n\ndef class_tversky(y_true, y_pred):\n    smooth = 1\n\n    y_true = K.permute_dimensions(y_true, (3,1,2,0))\n    y_pred = K.permute_dimensions(y_pred, (3,1,2,0))\n\n    y_true_pos = K.batch_flatten(y_true)\n    y_pred_pos = K.batch_flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos, 1)\n    false_neg = K.sum(y_true_pos * (1-y_pred_pos), 1)\n    false_pos = K.sum((1-y_true_pos)*y_pred_pos, 1)\n    alpha = 0.7\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\ndef focal_tversky_loss(y_true,y_pred):\n    pt_1 = class_tversky(y_true, y_pred)\n    gamma = 0.75\n    return K.sum(K.pow((1-pt_1), gamma))\n\ndef categorical_focal_loss_fixed(y_true, y_pred):\n    \"\"\"\n    :param y_true: A tensor of the same shape as `y_pred`\n    :param y_pred: A tensor resulting from a softmax\n    :return: Output tensor.\n    \"\"\"\n    alpha = .25\n    gamma = 2.\n\n    # Scale predictions so that the class probas of each sample sum to 1\n    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n\n    # Clip the prediction value to prevent NaN's and Inf's\n    epsilon = K.epsilon()\n    \n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    \n    # Calculate Cross Entropy\n    cross_entropy = -y_true * K.log(y_pred)\n\n    # Calculate Focal Loss\n    loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n\n    # Compute mean loss in mini_batch\n    return K.mean(K.sum(loss, axis=-1))\n\ndef combine_loss(y_true,y_pred):\n    tversky_loss = focal_tversky_loss(y_true,y_pred)\n    categorical_focal_loss = categorical_focal_loss_fixed(y_true,y_pred)\n    loss = tversky_loss + categorical_focal_loss\n    return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = (384,384,3)\nmodel = PSPNet(backbone_name='efficientnetb4',\n                   input_shape=IMG_SIZE,\n                   classes=4,\n                   activation='softmax',\n                   weights= None,\n                   encoder_weights='imagenet',\n                   encoder_freeze=True,\n                   downsample_factor=8,\n                   psp_conv_filters=512,\n                   psp_pooling_type='avg',\n                   psp_use_batchnorm=True,\n                   psp_dropout=0.25)\n\n# model = Unet(backbone_name='resnet34', \n#                          input_shape=(384, 384, 3), \n#                          classes=4, \n#                          activation='softmax', \n#                          weights=None, \n#                          encoder_weights='imagenet', \n#                          encoder_freeze=False, \n#                          encoder_features='default', \n#                          decoder_block_type='upsampling',\n#                          decoder_filters=(256, 128, 64, 32, 16), \n#                          decoder_use_batchnorm=True)\n#Load Pre Model If you had:\n#model.load_weights('../input/abcdef/efficientnetb31_epoch-006_loss-0.7743_val_loss-0.7764.h5')\n\n#List labels map \n#0-road, 1-line, 2-background, 3 - Traffic sign\n\n'''\nWeight of method-1\n------------------------------------\nWeight of Road:  0.3645078248104593\nWeight of Line:  0.028078782297801714\nWeight of BG:  0.5667032257035932\nWeight of Sign:  0.040710167188145796\n'''\n\n'''\nWeight of method-2\n------------------------------------\nWeight of Road:  0.3645078248104593\nWeight of Line:  0.02810491830366377\nWeight of BG:  0.5667032257035932\nWeight of Sign:  0.06350628595210751\n'''\n\n'''\n------------------------------------\nWeight of Road:  0.34606781118093866\nWeight of Line:  0.027605678363214644\nWeight of BG:  0.6213731314385866\nWeight of Sign:  0.011814638469397927\n'''\n\n'''\nWeight for image size: (384,384)\n------------------------------------\nWeight of Road:  0.36446377094056215\nWeight of Line:  0.028047958303599876\nWeight of BG:  0.5667768587313581\nWeight of Sign:  0.04071141202447992\n'''\n\n'''\nWeight for image size: (480,480)\n------------------------------------\nWeight of Road:  0.3646129968313012\nWeight of Line:  0.02808243137356203\nWeight of BG:  0.5665942663665875\nWeight of Sign:  0.04071030542854929\n'''\n\n'''\nWeight for image size: (144,144)\n------------------------------------\nWeight of Road:  0.36310559985228047\nWeight of Line:  0.027933321589439203\nWeight of BG:  0.5682527441486227\nWeight of Sign:  0.04070833440965764\n'''\nclass_weights_dice = [0.36446377094056215,0.028047958303599876,0.5667768587313581,0.04071141202447992]\nclass_indexes_dice = [0,1,2,3]\n# class_indexs_focal = [3,4,5,6,7,8]\n\n# --------------------------------------------------------------------\n#List of the loss value\njaccard_loss = JaccardLoss(class_weights=class_weights_dice, class_indexes=class_indexes_dice,per_image=True)\ndice_loss = DiceLoss(beta = 0.5,class_weights=class_weights_dice, class_indexes=class_indexes_dice) #khop anh \n\n# binary_focal_loss = BinaryFocalLoss()\ncategorical_focal_loss = CategoricalFocalLoss(alpha=0.25, gamma=2,class_indexes=[1])\n\n# binary_crossentropy = BinaryCELoss()\ncategorical_crossentropy = CategoricalCELoss(class_weights=class_weights_dice, class_indexes=class_indexes_dice)\n\n# loss combinations\n# bce_dice_loss = binary_crossentropy + dice_loss\n# bce_jaccard_loss = binary_crossentropy + jaccard_loss\n\ncce_dice_loss = categorical_crossentropy + dice_loss\ncce_jaccard_loss = categorical_crossentropy + jaccard_loss\n\n# binary_focal_dice_loss = binary_focal_loss + dice_loss\n# binary_focal_jaccard_loss = binary_focal_loss + jaccard_loss\n\ncategorical_focal_dice_loss = dice_loss + ( 1 * categorical_focal_loss )  # Dat Vu edited here\ncategorical_focal_jaccard_loss = categorical_focal_loss + jaccard_loss \n#------------------------------------------------------------------------\n\nmetrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5),Precision(threshold=0.5), Recall(threshold=0.5)]\n\n\n\nmodel.compile(optimizer=RMSprop(lr=0.0001), loss = cce_jaccard_loss, metrics = [metrics])\n\n#tb = TensorBoard(log_dir='logs', write_graph=True)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 10\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(mode='auto', filepath='./efficientnetb4_epoch-{epoch:03d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5', \n                     monitor='val_loss',  \n                     save_weights_only='True', \n                     period=1,\n                     verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                               min_delta=0.0,\n                               patience=10,\n                               verbose=1)\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                         factor=0.2,\n                                         patience=6,\n                                         verbose=1,\n                                         epsilon=0.001,\n                                         cooldown=0,\n                                         min_lr=0.00001)\n    \nsteps_per_epoch = np.ceil(float(train_numbers) / float(BATCH_SIZE))\nvalidation_steps = np.ceil(float(val_numbers) / float(BATCH_SIZE))\nresult = model.fit_generator(TrainAugmentGenerator(batch_size=BATCH_SIZE), steps_per_epoch=steps_per_epoch,\n                validation_data = ValAugmentGenerator(batch_size=BATCH_SIZE), \n                validation_steps = validation_steps, epochs=EPOCHS,callbacks=[model_checkpoint,early_stopping,reduce_learning_rate])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\n\nplt.subplot(121)\n\n# Get training and test loss histories\ntraining_loss = result.history['loss']\ntest_loss = result.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Model loss')\n\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\n# Pylot iou-score \nplt.subplot(122)\n\n# Get training and test loss histories\ntraining_loss = result.history['iou_score']\ntest_loss = result.history['val_iou_score']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Model iou_score')\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Score', 'Val IoU Score'])\nplt.xlabel('Epoch')\nplt.ylabel('IoU Score')\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n### Unfreeze layers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Custom loss\n## Lovasz Softmax","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nLovasz-Softmax and Jaccard hinge loss in Tensorflow\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n\"\"\"\n\nfrom __future__ import print_function, division\n\nimport tensorflow as tf\nimport numpy as np\n\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=\"loss\"\n                   )\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vscores, vlabels\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None, order='BHWC'):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [B, H, W, C] or [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n      order: use BHWC or BCHW\n    \"\"\"\n    print(probas,labels)\n    if per_image:\n        def treat_image(prob_lab):\n            prob, lab = prob_lab\n            prob, lab = tf.expand_dims(prob, 0), tf.expand_dims(lab, 0)\n            prob, lab = flatten_probas(prob, lab, ignore, order)\n            return lovasz_softmax_flat(prob, lab, classes=classes)\n        losses = tf.map_fn(treat_image, (probas, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore, order), classes=classes)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, classes='present'):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n    \"\"\"\n    C = 1\n    losses = []\n    present = []\n    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n    for c in class_to_sum:\n        fg = tf.cast(tf.equal(labels, c), probas.dtype)  # foreground for class c\n        if classes == 'present':\n            present.append(tf.reduce_sum(fg) > 0)\n        errors = tf.abs(fg - probas[:, c])\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort_{}\".format(c))\n        fg_sorted = tf.gather(fg, perm)\n        grad = lovasz_grad(fg_sorted)\n        losses.append(\n            tf.tensordot(errors_sorted, tf.stop_gradient(grad), 1, name=\"loss_class_{}\".format(c))\n                      )\n    if len(class_to_sum) == 1:  # short-circuit mean when only one class\n        return losses[0]\n    losses_tensor = tf.stack(losses)\n    if classes == 'present':\n        present = tf.stack(present)\n        losses_tensor = tf.boolean_mask(losses_tensor, present)\n    loss = tf.reduce_mean(losses_tensor)\n    return loss\n\n\ndef flatten_probas(probas, labels, ignore=None, order='BHWC'):\n    \"\"\"\n    Flattens predictions in the batch\n    \"\"\"\n    if order == 'BCHW':\n        probas = tf.transpose(probas, (0, 2, 3, 1), name=\"BCHW_to_BHWC\")\n        order = 'BHWC'\n    if order != 'BHWC':\n        raise NotImplementedError('Order {} unknown'.format(order))\n    C = 1\n    probas = tf.reshape(probas, (-1, C))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return probas, labels\n    valid = tf.not_equal(labels, ignore)\n    vprobas = tf.boolean_mask(probas, valid, name='valid_probas')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vprobas, vlabels\n\ndef keras_lovasz_softmax(labels,probas):\n    #return lovasz_softmax(probas, labels)+binary_crossentropy(labels, probas)\n    return lovasz_softmax(probas, labels,per_image=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = PSPNet(backbone_name='efficientnetb4',\n                   input_shape=(384, 384, 3),\n                   classes=4,\n                   activation='softmax',\n                   weights= './efficientnetb4_epoch-010_loss-0.7882_val_loss-0.7813.h5',\n                   encoder_weights='imagenet',\n                   encoder_freeze=False,\n                   downsample_factor=8,\n                   psp_conv_filters=512,\n                   psp_pooling_type='avg')\n#                    psp_use_batchnorm=True,\n#                    psp_dropout=0.25)\nfor layer in model.layers:\n    layer.trainable = True\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 20 # Update Epochs\nBATCH_SIZE = 16\nsteps_per_epoch = np.ceil(float(train_numbers) / float(BATCH_SIZE))\nvalidation_steps = np.ceil(float(val_numbers) / float(BATCH_SIZE))\nmodel.compile(optimizer=RMSprop(lr=0.0001), loss = cce_jaccard_loss, metrics = [metrics])\n\nresult = model.fit_generator(TrainAugmentGenerator(batch_size=BATCH_SIZE), steps_per_epoch=steps_per_epoch,\n                validation_data = ValAugmentGenerator(batch_size=BATCH_SIZE), \n                validation_steps = validation_steps, epochs=EPOCHS,callbacks=[model_checkpoint,early_stopping,reduce_learning_rate])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\n\nplt.subplot(121)\n\n# Get training and test loss histories\ntraining_loss = result.history['loss']\ntest_loss = result.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Model loss')\n\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\n# Pylot iou-score \nplt.subplot(122)\n\n# Get training and test loss histories\ntraining_loss = result.history['iou_score']\ntest_loss = result.history['val_iou_score']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Model iou_score')\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Score', 'Val IoU Score'])\nplt.xlabel('Epoch')\nplt.ylabel('IoU Score')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get training and test loss histories\ntraining_loss = result.history['f1-score']\ntest_loss = result.history['val_f1-score']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Score', 'Val F1-Score'])\nplt.xlabel('Epoch')\nplt.ylabel('F1 Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\n\nplt.subplot(121)\n\n# Get training and test loss histories\ntraining_loss = result.history['precision']\ntest_loss = result.history['val_precision']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Model Precision')\n\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Presicion', 'Test Presicion'])\nplt.xlabel('Epoch')\nplt.ylabel('Presicion Score')\n\n# Pylot iou-score \nplt.subplot(122)\n\n# Get training and test loss histories\ntraining_loss = result.history['recall']\ntest_loss = result.history['val_recall']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Model Recall')\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Score', 'Val Recall Score'])\nplt.xlabel('Epoch')\nplt.ylabel('IoU Score')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stage 3: \nTraining with Weighted Cross-Entropy + Dice Loss with beta = 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dice_loss = DiceLoss(beta = 1,class_weights=class_weights_dice, class_indexes=class_indexes_dice) #khop anh \n\ncategorical_crossentropy = CategoricalCELoss(class_weights=class_weights_dice, class_indexes=class_indexes_dice)\n\ncce_dice_loss = categorical_crossentropy + dice_loss\n\nmodel = PSPNet(backbone_name='efficientnetb4',\n                   input_shape=(384, 384, 3),\n                   classes=4,\n                   activation='softmax',\n                   weights= './efficientnetb4_epoch-020_loss-0.7747_val_loss-0.7731.h5',\n                   encoder_weights='imagenet',\n                   encoder_freeze=False,\n                   downsample_factor=8,\n                   psp_conv_filters=512,\n                   psp_pooling_type='avg')\n#                    psp_use_batchnorm=True,\n#                    psp_dropout=0.25)\nfor layer in model.layers:\n    layer.trainable = True\n\nmodel.summary()\n\nEPOCHS = 30 # Update Epochs\nBATCH_SIZE = 16\nsteps_per_epoch = np.ceil(float(train_numbers) / float(BATCH_SIZE))\nvalidation_steps = np.ceil(float(val_numbers) / float(BATCH_SIZE))\nmodel.compile(optimizer=RMSprop(lr=0.0001), loss = cce_jaccard_loss, metrics = [metrics])\n\nresult = model.fit_generator(TrainAugmentGenerator(batch_size=BATCH_SIZE), steps_per_epoch=steps_per_epoch,\n                validation_data = ValAugmentGenerator(batch_size=BATCH_SIZE), \n                validation_steps = validation_steps, epochs=EPOCHS,callbacks=[model_checkpoint,early_stopping,reduce_learning_rate])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict Video","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = PSPNet(backbone_name='efficientnetb4',\n                   input_shape=(384, 384, 3),\n                   classes=4,\n                   activation='softmax',\n                   weights= None,\n                   encoder_weights='imagenet',\n                   encoder_freeze=False,\n                   downsample_factor=8,\n                   psp_conv_filters=512,\n                   psp_pooling_type='avg',\n                   psp_use_batchnorm=True,\n                   psp_dropout=0.25)\n\nmodel.load_weights('./efficientnetb4_epoch-020_loss-0.7733_val_loss-0.7726.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import interpolate\nfrom scipy.interpolate import RectBivariateSpline\n\ndef onehot_to_rgb(onehot, colormap):\n    '''Function to decode encoded mask labels\n        Inputs: \n            onehot - one hot encoded image matrix (height x width x num_classes)\n            colormap - dictionary of color to label id\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros((384,384,3))\n    for k in colormap.keys():\n        output[single_layer==k] = colormap[k]\n    # Interplotation\n#     line = np.zeros((144,144))\n#     line[single_layer == 1] = 255    \n    \n# #     indices = np.array([[0, 0]])\n#     x = np.array([],dtype = np.int8)\n#     y = np.array([],dtype = np.int8)\n#     for i in range(0,144):\n#         for j in range(0,144):\n#             if line[i][j] == 255:\n# #                 arr = np.array([[j,i]])\n# #                 indices = np.append(indices,arr,axis=0)\n#                 x = np.append(x,[j],axis=0)\n#                 y = np.append(y,[i],axis=0)\n# #     print(x[0],y[0])\n# #     indices = np.delete(indices, 0, axis=0) # Remove first element\n\n#     M, N = line.shape\n    \n#     spline = RectBivariateSpline(np.arange(M), np.arange(N), line)\n#     interpolated = spline(x, y, grid=False)\n    \n#     print(interpolated)\n#     nans = np.zeros(interpolated.shape) + np.nan\n#     #print(nans)\n#     x_in_bounds = (0 <= x) & (x < M)\n#     y_in_bounds = (0 <= y) & (y < N)\n#     bounded = np.where(x_in_bounds & y_in_bounds, interpolated, nans)\n#     #print(bounded)\n\n    \n#     xx, yy = np.meshgrid(x, y)\n#     z = np.sin(xx**2+yy**2)\n#     f = interpolate.interp2d(x, y, z, kind='cubic')\n    \n#     for i in range(0,144):\n#         for j in range(0,144):\n#             if line[i][j] != 255:\n#                 znew = f(j, i)\n#                 if znew != null:\n#                     print(znew)\n                \n                \n\n#     line = np.zeros((144,144))\n#     line[single_layer == 1] = 255\n#     line = output[72:,:]\n#     for i in range(0,72):\n#         for j in range(0,144):\n#             if line_opencv[i][j] == 255:\n#                 line[i][j] = 255\n    return np.uint8(output)#,np.uint8(line)\n\n\nimport cv2\nimport numpy as np\ncapture = cv2.VideoCapture('../input/videoo/Video_Beta.avi')\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('./output_2007_v4_3.avi',fourcc, 30.0, (384,384))\ni=0\nwhile(True):\n    ret = capture.grab()\n    #if i % 10 == 0:\n        \n    ret, frame = capture.retrieve()\n    \n    img = cv2.resize(frame,(384,384),interpolation=cv2.INTER_NEAREST)\n    \n    lower = np.array([0, 0, 200])\n    upper = np.array([180, 255, 255])\n    # Create HSV Image and threshold into a range.\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    mask = cv2.inRange(hsv, lower, upper)\n    line_opencv = mask[72:,:] # We want to get half of image below\n    \n    img = (img[...,::-1].astype(np.float32)) / 255.0\n    img = np.reshape(img, (1, 384, 384, 3))\n    predict_one = model.predict(img)\n    \n    #frame_real = cv2.resize(frame,(144,144),interpolation=cv2.INTER_NEAREST)\n    frame_pre = onehot_to_rgb(predict_one[0],id2code)\n    #cv2.imwrite('test.png',frame_pre)\n#     line = cv2.resize(line,(144,144))\n    \n    #frame_total = np.concatenate((frame_real, frame_pre), axis=0)\n    \n    i += 1\n    print(\"Frame thu: \",i)\n    #GG\n    #cv2.imshow(\"Test\",frame_total)\n    #Save video\n    out.write(frame_pre)\n    #cv2.imwrite('Frame_'+ str(i) +'.png',frame_total)\n        # do something with frame\ncapture.release()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Post Processing\n\nStep 1: Cubic spine interpolation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import interpolate\nx = np.arange(-5.01, 5.01, 0.25)\ny = np.arange(-5.01, 5.01, 0.25)\nxx, yy = np.meshgrid(x, y)\nz = np.sin(xx**2+yy**2)\nf = interpolate.interp2d(x, y, z, kind='cubic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xnew = np.arange(-5.01, 5.01, 1e-2)\nynew = np.arange(-5.01, 5.01, 1e-2)\nznew = f(xnew, ynew)\nplt.plot(x, z[0, :], 'ro-', xnew, znew[0, :], 'b-')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert model to .pb frozen graph ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n# This line must be executed before loading Keras model.\nK.set_learning_phase(0)\nimport tensorflow as tf\n\ndef freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n    \"\"\"\n    Freezes the state of a session into a pruned computation graph.\n\n    Creates a new computation graph where variable nodes are replaced by\n    constants taking their current value in the session. The new graph will be\n    pruned so subgraphs that are not necessary to compute the requested\n    outputs are removed.\n    @param session The TensorFlow session to be frozen.\n    @param keep_var_names A list of variable names that should not be frozen,\n                          or None to freeze all the variables in the graph.\n    @param output_names Names of the relevant graph outputs.\n    @param clear_devices Remove the device directives from the graph for better portability.\n    @return The frozen graph definition.\n    \"\"\"\n    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n    graph = session.graph\n    with graph.as_default():\n        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n        output_names = output_names or []\n        output_names += [v.op.name for v in tf.global_variables()]\n        # Graph -> GraphDef ProtoBuf\n        input_graph_def = graph.as_graph_def()\n        if clear_devices:\n            for node in input_graph_def.node:\n                node.device = \"\"\n        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n                                                      output_names, freeze_var_names)\n        return frozen_graph\n    \nmodel.load_weights('/home/dylan/Desktop/Goodgame_Training/model/Model_4_class_Test_1_epoch-175_loss-0.7673_val_loss-0.7632.h5')\n\nfrozen_graph = freeze_session(K.get_session(),\n                              output_names=[out.op.name for out in model.outputs])\ntf.train.write_graph(frozen_graph, \"model_pb\", \"Model_4_class_Test_1_epoch-175_loss-0.7673_val_loss-0.7632.pb\", as_text=False)\n\nprint(\"Input Model:  \",model.inputs)\nprint(\"Output Model:  \",model.outputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(16,10))\n\nimg = cv2.imread('../input/preimages/data_beta_bouns(319).png')\nimg = cv2.resize(img,(144,144))\nplt.subplot(1,2,1)\nplt.imshow(img)\nlower = np.array([0, 0, 220])\nupper = np.array([180, 255, 255])\n# Create HSV Image and threshold into a range.\nhsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\nmask = cv2.inRange(hsv, lower, upper)\nline_opencv = mask[72:,:] # We want to get half of image below\n#print(line_opencv.shape)\nplt.subplot(1,2,2)\nplt.imshow(line_opencv,cmap=\"gray\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_to_rgb(onehot, colormap,line_opencv):\n    '''Function to decode encoded mask labels\n        Inputs: \n            onehot - one hot encoded image matrix (height x width x num_classes)\n            colormap - dictionary of color to label id\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros((144,144))\n#     for k in colormap.keys():\n#         output[single_layer==k] = colormap[k]\n    output[single_layer == 1] = 255\n    output = output[72:,:]\n    for i in range(0,72):\n        for j in range(0,144):\n            if line_opencv[i][j] == 255:\n                output[i][j] = 255\n    return np.uint8(output)\n\nframe = cv2.imread('../input/preimages/data_beta_bouns(319).png')\nimg = cv2.resize(frame,(144,144))\nimg = (img[...,::-1].astype(np.float32)) / 255.0\nimg = np.reshape(img, (1, 144, 144, 3))\npredict_one = model.predict(img)\n\nframe_real = cv2.resize(frame,(144,144))\nframe_pre = onehot_to_rgb(predict_one[0],id2code,line_opencv)\n\ncv2.imwrite('./line.png',frame_pre)\n#frame_total = np.concatenate((frame_real, frame_pre), axis=1)\nplt.imshow(frame_pre,cmap=\"gray\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}